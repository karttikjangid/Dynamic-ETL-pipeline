(read guidelines.md for more context)
# **`guidelines_B.md` â€” Dynamic ETL Pipeline (Tier A â†’ Tier B Upgrade)**

### **Version: 2.0 â€” Tierâ€‘B Ready, with SQLite, NER, DeepDiff, and Genson Versioning**

---

# ğŸš€ **1. Mission**

Build a **deterministic, schemaâ€‘evolving ETL pipeline** for unstructured data spanning **Tierâ€‘A and Tierâ€‘B** complexity:

* Tierâ€‘A: JSON, KV, Markdown.
* Tierâ€‘B: messy mixed content (HTML snippets, CSVâ€‘like fragments, partial structures, semiâ€‘structured sections).

The system must automatically ingest â†’ extract â†’ normalize â†’ infer schema â†’ version using **DeepDiff + Genson** â†’ store records in **SQLite** â†’ run strict DB queries.

**Goals:** reliability, determinism, incremental schema evolution, transparent diffs, strong evidence reporting.

**Nonâ€‘goals:** naturalâ€‘language queries, PDFs/OCR (Tierâ€‘C), complex distributed storage.

---

# ğŸ§© **2. Endâ€‘toâ€‘end pipeline (Tier A + Tier B)**

1. **Upload â†’ /upload**

   * Accepts `.txt` /.md`+ Tierâ€‘B`.txt` containing mixed fragments.
   * Validates MIME & extension.

2. **Extraction layer**

   * Tierâ€‘A Extractors: JSON / KV / fenced blocks.
   * Tierâ€‘B Extractors: HTML snippet detector, CSV heuristics, YAML locator, NER enrichment.
   * Emits **fragment stats**: `{json_fragments, kv_pairs, html_tables, csv_blocks, yaml_blocks, total_records}`.
   * Each extraction includes **offsets**, **token spans**, and **source-type classification**.

3. **Normalization**

   * JSONNormalizer, KVNormalizer, CSVNormalizer, HTMLTableNormalizer.
   * NER attached per fragment: entities = `{PERSON, ORG, PRODUCT, LOCATION, DATE}`.

4. **Schema Inference** using **Genson + custom layer**

   * Genson provides base JSON-schema aggregation.
   * Custom layer attaches: type-confidence, nullable, path, semantic hints.

5. **Schema Versioning** using **DeepDiff + Genson**

   * Compute signature from Genson canonical schema.
   * Compare previous â†’ new via DeepDiff.
   * If diff only cosmetic â†’ keep same version.
   * If structural â†’ increment `schema_v{n+1}`.

6. **Storage â†’ SQLite**

   * Flatten nested fields to SQLite columns.
   * Mixed arrays stored as JSON.
   * Schema change â†’ creates a new SQLite table with name `{source_id}_v{schema_version}`.

7. **Queries â†’ /query**

   * Accepts only SQLâ€‘safe structured queries: `{select, where, limit, order_by}`.
   * Routes to correct SQLite table based on schema version.

---

# ğŸ§± **3. Schema metadata fields (completed spec)**

Every schema must include:

```
schema_id
source_id
generated_at
compatible_dbs: ["sqlite"]
version
fields: [
  name
  path
  type
  nullable
  example_value
  confidence
  source_offsets
  suggested_index
]
primary_key_candidates
migration_notes
version_diff (DeepDiff output)
```

---

# ğŸ“¦ **4. Evidence & gaps output (Completed Template)**

Every phase must return a structured evidence block:

| Phase                             | Status summary                           | Evidence & gaps                                                              |
| --------------------------------- | ---------------------------------------- | ---------------------------------------------------------------------------- |
| **0 â€“ Sanity**                    | Fully validated                          | Endpoints live. MIME accepted. SQLite connected.                             |
| **1 â€“ Ingest & Parse (Tier A/B)** | JSON/KV/HTML/CSV extracted with offsets  | Missing advanced CSV dialect detection. HTML parser handles tables only.     |
| **2 â€“ Schema Generation**         | Genson + custom metadata                 | Type mapping improved; SQL type mapping complete; PK suggestions available.  |
| **3 â€“ Evolving Schema**           | DeepDiff detects adds/removes/type flips | No field-regression testing yet.                                             |
| **4 â€“ Type-change & Ambiguity**   | Mixed-type detection working             | Normalization plan uses union-type mapping; needs richer datetime inference. |
| **5 â€“ Mapping to Target DB**      | SQLite SQL DDL emitted                   | No multi-DB export.                                                          |

This evidence block must be included in each `/upload` response.

---

# ğŸ§ª **5. Tierâ€‘B Extractors â€” required behaviours**

### âœ” HTML snippet extraction

* Detect `<table>...</table>`
* Convert to row arrays.
* Attach `html_table_id` + offsets.

### âœ” CSV block detection

* Regex-based row alignment.
* Infer header.
* Convert to records.

### âœ” Mixed-fragment ties

* Each fragment must include: `fragment_id`, `source_type`, `offset_range`, `cleaned_text`.

---

# ğŸ”§ **6. Versioning Model (DeepDiff + Genson)**

```
schema_raw = genson_schema
prev_schema_raw = last_schema.genson_version

changes = DeepDiff(prev_schema_raw, schema_raw)

if changes.is_empty():
    version stays same
else:
    increment version
    attach `version_diff = changes.to_dict()`
```

---

# ğŸ› **7. SQLite Storage Rules**

* Each schema version gets its own table.
* Table name: `{source_id}_v{schema_version}`.
* Columns generated from flattened schema paths.
* Array â†’ JSON text.
* Indexes created for all `suggested_index=True` fields.

---

# ğŸ“¡ **8. APIs**

## `/upload`

Returns:

```
{
  status,
  source_id,
  file_id,
  schema_id,
  version,
  parsed_fragments_summary,
  evidence,
  schema_metadata
}
```

## `/schema`

Returns latest schema.

## `/schema/history`

Shows all versions and DeepDiff diffs.

## `/query`

Strict structured SQL DSL only.

---

# ğŸ” **9. Principles**

* Deterministic.
* Schema evolution = diff-based, not time-based.
* Evidence-first: every step produces measurable stats.
* Extraction never fails silently.
* Normalization produces reversible mapping.
* Storage is versioned, never destructive.
* Queries are strict and safe.

---

# ğŸ§­ **10. Developer Workflow**

* Python 3.11
* SQLite (built-in)
* pip install: `genson`, `deepdiff`, `beautifulsoup4`, `python-dateutil`, `spacy`, `pandas`
* Run: `python -m spacy download en_core_web_sm`

---